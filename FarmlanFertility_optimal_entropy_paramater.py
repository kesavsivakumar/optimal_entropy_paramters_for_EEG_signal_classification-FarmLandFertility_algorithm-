# -*- coding: utf-8 -*-
"""Copy of EEG Data anlaysis_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tMpCmhlBN18kk3ElsGcLKeRmCzxrosnL
"""

from google.colab import drive
drive.mount('/content/drive')

cd drive

cd My Drive

import os
import numpy as np
import pywt 
from scipy.stats import entropy as kl
import matplotlib.pyplot as plt
import time
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score

from numpy import arange
from sklearn.datasets import make_classification
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import normalize
from sklearn.preprocessing import MinMaxScaler

import pandas as pd
from scipy.stats import chi2
from scipy.stats import chisquare
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind
import seaborn as sns
from sklearn.metrics import confusion_matrix

def bonn_dataset(data,wavelet_family = 'db10', level = 4):

    #data = np.load("../Healthcare_signal_processing/Datasets/Bonn/data_all.npz")
    db = pywt.Wavelet(wavelet_family)
    a4 = []; d4 = []; d3 = []; d2 = []; d1 = []
    for samp in data:
      cA4, cD4, cD3, cD2, cD1 = pywt.wavedec(samp, db, level = level)
      a4.append(cA4)
      d4.append(cD4)
      d3.append(cD3)
      d2.append(cD2)
      d1.append(cD1)
      #print(samp.shape,len(cA4),len(cD4),len(d3),len(d2),len(d1))

    a4 = np.array(a4)
    d4 = np.array(d4)
    d3 = np.array(d3)
    d2 = np.array(d2)
    d1 = np.array(d1)
    print("[INFO] Dataset processing completed")
    return [a4, d4, d3, d2, d1]

!pip install pyentrp
!pip install entropy
!pip install EntroPy-Package

from pyentrp import entropy as ent
import numpy as np
import entropy
from scipy.special import gamma,psi
from scipy import ndimage
from scipy.linalg import det
from numpy import pi
from sklearn.neighbors import NearestNeighbors

data=[]
a_dir='/content/drive/My Drive/Z_CLASS'
a=[]
for path in os.listdir(a_dir):
  #print(path)
  f=open(a_dir+'/'+path,'r')
  a.append(list(map(str,f.read().split('\n'))))

for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)


e_dir='/content/drive/My Drive/S_CLASS'
e=[]
for path in os.listdir(e_dir):
  #print(path)
  f=open(e_dir+'/'+path,'r')
  e.append(list(map(str,f.read().split('\n'))))
  
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)

b_dir='/content/drive/My Drive/O_CLASS'
b=[]
for path in os.listdir(b_dir):
  #print(path)
  f=open(b_dir+'/'+path,'r')
  b.append(list(map(str,f.read().split('\n'))))

c_dir='/content/drive/My Drive/N_CLASS'
c=[]
for path in os.listdir(c_dir):
  #print(path)
  f=open(c_dir+'/'+path,'r')
  c.append(list(map(str,f.read().split('\n'))))

d_dir='/content/drive/My Drive/F_CLASS'
d=[]
for path in os.listdir(d_dir):
  #print(path)
  f=open(d_dir+'/'+path,'r')
  d.append(list(map(str,f.read().split('\n'))))

A_dat=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  A_dat.append(temp)

A_dat=np.array(A_dat)
Aclass=A_dat.flatten()

B_dat=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  B_dat.append(temp)

B_dat=np.array(B_dat)
Bclass=B_dat.flatten()


B_dat=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  B_dat.append(temp)

B_dat=np.array(B_dat)
Bclass=B_dat.flatten()


B_dat=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  B_dat.append(temp)

B_dat=np.array(B_dat)
Bclass=B_dat.flatten()


C_dat=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  C_dat.append(temp)

C_dat=np.array(C_dat)
Cclass=C_dat.flatten()


D_dat=[]
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  D_dat.append(temp)

D_dat=np.array(D_dat)
Dclass=D_dat.flatten()


E_dat=[]
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  E_dat.append(temp)

E_dat=np.array(E_dat)
Eclass=E_dat.flatten()

Bclass

def DWT(data,wavelet_family = 'db10', level = 4):

    #data = np.load("../Healthcare_signal_processing/Datasets/Bonn/data_all.npz")
    db = pywt.Wavelet(wavelet_family)
    a4 = []; d4 = []; d3 = []; d2 = []; d1 = []
    for samp in data:
      cA4, cD4, cD3, cD2, cD1 = pywt.wavedec(samp, db, level = level)
      a4.append(cA4)
      d4.append(cD4)
      d3.append(cD3)
      d2.append(cD2)
      d1.append(cD1)
      #print(samp.shape,len(cA4),len(cD4),len(d3),len(d2),len(d1))

    a4 = np.array(a4)
    d4 = np.array(d4)
    d3 = np.array(d3)
    d2 = np.array(d2)
    d1 = np.array(d1)
    print("[INFO] Dataset processing completed")
    return [a4, d4, d3, d2, d1]

A_DWT=DWT([Aclass])
B_DWT=DWT([Bclass])
C_DWT=DWT([Cclass])
D_DWT=DWT([Dclass])
E_DWT=DWT([Eclass])

df={'A4':list(A_DWT[0][0]),'D4':list(A_DWT[1][0]),'D3': list(A_DWT[2][0]),
                    'D2':list(A_DWT[3][0]),'D1':list(A_DWT[4][0])}
Output = pd.DataFrame.from_dict(df, orient='index')

Output.to_csv('DB10_CLASS_A.csv',index=False)

df={'A4':list(B_DWT[0][0]),'D4':list(B_DWT[1][0]),'D3': list(B_DWT[2][0]),
                    'D2':list(B_DWT[3][0]),'D1':list(B_DWT[4][0])}
Output = pd.DataFrame.from_dict(df, orient='index')

Output.to_csv('DB10_CLASS_B.csv',index=False)

df={'A4':list(C_DWT[0][0]),'D4':list(C_DWT[1][0]),'D3': list(C_DWT[2][0]),
                    'D2':list(C_DWT[3][0]),'D1':list(C_DWT[4][0])}
Output = pd.DataFrame.from_dict(df, orient='index')

Output.to_csv('DB10_CLASS_C.csv',index=False)

df={'A4':list(D_DWT[0][0]),'D4':list(D_DWT[1][0]),'D3': list(D_DWT[2][0]),
                    'D2':list(D_DWT[3][0]),'D1':list(D_DWT[4][0])}
Output = pd.DataFrame.from_dict(df, orient='index')

Output.to_csv('DB10_CLASS_D.csv',index=False)

df={'A4':list(E_DWT[0][0]),'D4':list(E_DWT[1][0]),'D3': list(E_DWT[2][0]),
                    'D2':list(E_DWT[3][0]),'D1':list(E_DWT[4][0])}
Output = pd.DataFrame.from_dict(df, orient='index')

Output.to_csv('DB10_CLASS_E.csv',index=False)

"""Sample Plot"""

A_dat[0]

A_DWT=DWT([A_dat[0]])
B_DWT=DWT([B_dat[0]])
C_DWT=DWT([C_dat[0]])
D_DWT=DWT([D_dat[0]])
E_DWT=DWT([E_dat[0]])

df={'A4':list(A_DWT[0][0]),'D4':list(A_DWT[1][0]),'D3': list(A_DWT[2][0]),
                    'D2':list(A_DWT[3][0]),'D1':list(A_DWT[4][0])}
Output = pd.DataFrame.from_dict(df, orient='index')

Output.to_csv('DB10_CLASS_A_sample.csv',index=False)

df={'A4':list(B_DWT[0][0]),'D4':list(B_DWT[1][0]),'D3': list(B_DWT[2][0]),
                    'D2':list(B_DWT[3][0]),'D1':list(B_DWT[4][0])}
Output = pd.DataFrame.from_dict(df, orient='index')

Output.to_csv('DB10_CLASS_B_sample.csv',index=False)

df={'A4':list(C_DWT[0][0]),'D4':list(C_DWT[1][0]),'D3': list(C_DWT[2][0]),
                    'D2':list(C_DWT[3][0]),'D1':list(C_DWT[4][0])}
Output = pd.DataFrame.from_dict(df, orient='index')

Output.to_csv('DB10_CLASS_C_sample.csv',index=False)

df={'A4':list(D_DWT[0][0]),'D4':list(D_DWT[1][0]),'D3': list(D_DWT[2][0]),
                    'D2':list(D_DWT[3][0]),'D1':list(D_DWT[4][0])}
Output = pd.DataFrame.from_dict(df, orient='index')

Output.to_csv('DB10_CLASS_D_sample.csv',index=False)

df={'A4':list(E_DWT[0][0]),'D4':list(E_DWT[1][0]),'D3': list(E_DWT[2][0]),
                    'D2':list(E_DWT[3][0]),'D1':list(E_DWT[4][0])}
Output = pd.DataFrame.from_dict(df, orient='index')

Output.to_csv('DB10_CLASS_E_sample.csv',index=False)

"""PLOT"""

groups = [0, 1, 2, 3,4]
columns=['A - A4','A - D4','A - D3','A - D2','A - D1']
i = 1
#plt.title('class A')
plt.figure(figsize=(20,10))
plt.title('class A',loc='center')
for group in groups:
    plt.subplot(len(groups), 1, i)
    plt.plot(A_DWT[group][0])
    plt.title(columns[group], y=0.5, loc='right')
    i += 1

plt.show()

groups = [0, 1, 2, 3,4]
columns=['B - A4','B - D4','B - D3','B - D2','B - D1']
i = 1
#plt.title('class A')
plt.figure(figsize=(20,10))
plt.title('class B',loc='center')
for group in groups:
    plt.subplot(len(groups), 1, i)
    plt.plot(A_DWT[group][0])
    plt.title(columns[group], y=0.5, loc='right')
    i += 1

plt.show()

groups = [0, 1, 2, 3,4]
columns=['C - A4','C - D4','C - D3','C - D2','C - D1']
i = 1
#plt.title('class A')
plt.figure(figsize=(20,10))
plt.title('class C',loc='center')
for group in groups:
    plt.subplot(len(groups), 1, i)
    plt.plot(A_DWT[group][0])
    plt.title(columns[group], y=0.5, loc='right')
    i += 1

plt.show()

groups = [0, 1, 2, 3,4]
columns=['D - A4','D - D4','D - D3','D - D2','D - D1']
i = 1
#plt.title('class A')
plt.figure(figsize=(20,10))
plt.title('class D',loc='center')
for group in groups:
    plt.subplot(len(groups), 1, i)
    plt.plot(A_DWT[group][0])
    plt.title(columns[group], y=0.5, loc='right')
    i += 1

plt.show()

groups = [0, 1, 2, 3,4]
columns=['E - A4','E - D4','E - D3','E - D2','E - D1']
i = 1
#plt.title('class A')
plt.figure(figsize=(20,10))
plt.title('class E',loc='center')
for group in groups:
    plt.subplot(len(groups), 1, i)
    plt.plot(A_DWT[group][0])
    plt.title(columns[group], y=0.5, loc='right')
    i += 1

plt.show()

sns.distplot(chisquare(data[0:100].flatten()), hist=False)
sns.distplot(chisquare(B_dat.flatten()), hist=False)
plt.title("Chi square Distribution")
plt.legend(loc='upper right',labels=['A','B'])
plt.show()

ttest_ind(data[0:100].flatten(),data[100:].flatten())

D=bonn_dataset(data)

len(D)

import sys
import time

def renyi_entropy(data, alpha):
    '''if alpha ==1:
        alpha=0'''
        #alpha = np.random.uniform(0.9,1.1)
        #reyni_entropy(data,alpha)
    ren_ent = []
    for i in range(data.shape[0]):
        _, ele_counts = np.unique( np.around(data[i], decimals = 0 ), return_counts = True )
        summation = np.sum( np.log( ( ele_counts / data.shape[1] ) ** alpha ) ) 
        ren_ent.append( (1 / (1 - alpha)) * summation )
    return np.array(ren_ent)

def permutation_ent(data, order):
    if order < 2:
        order = 2
    perm_ent = []
    #order = int(order)
    
    for i in range(data.shape[0]):
        data[i] = np.around(data[i], decimals = 0)
        perm_ent.append(ent.permutation_entropy(data[i], order = order, normalize = True))
    return np.array(perm_ent)

def tsallis_ent(data, alpha):
    '''if alpha ==1:
        alpha=0'''
        #alpha = np.random.uniform(0.9,1.1)
        #tsallis_ent(data,alpha)
    tsa_ent = []
    for i in range(data.shape[0]):
        _, ele_counts = np.unique( np.around(data[i], decimals = 0 ), return_counts = True )
        summation = np.sum( np.log( ( ele_counts / data.shape[1] ) ** alpha ) ) 
        tsa_ent.append( (1 / (alpha - 1)) * ( 1 - summation) )
    return np.array(tsa_ent)

def kraskov_ent(data, k):
    #if k < 1:
        #k = 1
    kra_ent = []
    #k=int(k)
    knn = NearestNeighbors(n_neighbors=k)
    for i in range(data.shape[0]):
        X = np.around(data[i], decimals = 0 ).reshape(-1,1)
        knn.fit(X)
        r, _ = knn.kneighbors(X)
        n, d = X.shape
        volume_unit_ball = (pi**(.5*d)) / gamma(.5*d + 1)    
        kra_ent.append((d*np.mean(np.log(r[:,-1] + np.finfo(X.dtype).eps))+ np.log(volume_unit_ball) + psi(n) - psi(k)))
    return np.array(kra_ent)

def approx_entropy(U, m, r): #m must be a whole number 
  approx_ent=[]
  for i in range(data.shape[0]):
    U = data[i]
    #print(U.shape)
    N = U.shape[0]
    def _phi(m):
      z = N - m + 1.0
      x = np.array([U[i:i+m] for i in range(int(z))])
      X = np.repeat(x[:, np.newaxis], 1, axis=2)
      C = np.sum(np.absolute(x - X).max(axis=2) <= r, axis=0) / z
      return np.log(C).sum() / z
    approx_ent.append(abs(_phi(m + 1) - _phi(m)))
  return np.array(approx_ent)

import random
def farm_land_fertility_compute_optim_RT(D,option,flag_entropy,output_file,max_iteration = 10):
  if option==0:
    print('=== REYNI ENTROPY ===')
  else:
    print('=== TSALLIS ENTROPY ===')
  n=3  ### no of samples in a sector
  k=4  ### no of sections
  
  population_size = 12  ### population size = n*k
  no_of_class = 2
  best_param=[]
  best_param_sof_far=[]
  int_pop = list(np.random.uniform(1.85, 2.2, size =population_size*len(D)))
  kl_values = [-999999999]*(population_size*len(D))
  global_best_kl=[-999999999]*len(D)
  global_best_order=[-999999999]*len(D)
  local_best_kl=[-9999999999]*(len(D)*k)
  local_best_order=[-9999999999]*(len(D)*k)
  mean_kl=[-999999999]*(k*len(D))
  worst_section_kl=[999999999]*len(D)
  worst_section=[-1]*len(D)
  #external_memory_order=[0]*(population_size*len(D-1))
  alpha = np.random.rand(1)[0]
  count=0
  w=np.random.rand(1)[0]
  q=np.random.rand(1)[0]
  copy_global_param=global_best_order
  f=open(output_file+'.txt','w')
  ent_exc_time=time.time()
  for iter in range(max_iteration):
    #print("ITERATION ",iter," ------------------------------")
    f.write("ITERATION "+str(iter)+" ------------------------------ \n")
    tic = time.time() 
    
    #calculating kl divergence for population
    for pop in range(population_size):
      for band in range(len(D)): 
        if option==0:
          int_pop=[1.8 if i <1.8 else i for i in int_pop]
          int_pop=[2.3 if i >2.3 else i for i in int_pop]
          ent=renyi_entropy(D[band],int_pop[pop*len(D)+band])
        else:
          int_pop=[1.8 if i <1.8 else i for i in int_pop]
          int_pop=[2.3 if i >2.3 else i for i in int_pop]
          ent=tsallis_ent(D[band],int_pop[pop*len(D)+band])
        a,e=np.split(ent,no_of_class)
        kl_values[pop*len(D)+band]=kl(a,e)

    #saving best kl and corresposnding order values
    for pop in range(population_size):
      for band in range(len(D)):
        if global_best_kl[band] < kl_values[(pop*5)+band]:
          global_best_kl[band]=kl_values[(pop*len(D))+band]
          global_best_order[band]=int_pop[(pop*len(D))+band]

    #saving local best kl and corresponding order
    for section in range(k):
      for samp in range(n):
        for band in range(len(D)):
          if local_best_kl[section*len(D)+band] < kl_values[section*samp+band]:
            local_best_kl[section*len(D)+band]=kl_values[section*samp+band]
            local_best_order[section*len(D)+band]=int_pop[section*samp+band]

    #calculating mean fitness
    len_sec=len(int_pop)//k
    for j in range(k):
      sliced_pop=kl_values[j*len_sec:(j*len_sec)+len_sec]
      x,y,z=sliced_pop[0:5],sliced_pop[5:10],sliced_pop[10:15]
      for band in range(len(D)):
        mean_kl[j*len(D)+band]=(x[band]+y[band]+z[band])/n
    
    #updating worst section kl
    for band in range(len(D)):
      for section in range(k):
        if mean_kl[section*len(D)+band] < worst_section_kl[band]:
          worst_section_kl[band]= mean_kl[section*len(D)+band]
          worst_section[band]=section
    
    
    # rearranging population into array (just for convenience)
    h=alpha*np.random.uniform(-1,1)
    coeff=np.array([[0]*population_size]*len(D))
    for pop in range(population_size):
      for band in range(len(D)): 
        coeff[band][pop]=int_pop[pop*len(D)+band]
    # udpdating population for next step
    for band in range(coeff.shape[0]):
      for pop in range(coeff.shape[1]):
        sec=pop//n
        if sec==worst_section[band]:
          l=list(coeff[band])
          del l[sec*n:(sec+1)*n]
          random_soln=random.choice(l)    ### selecting a random solution from external memory
          coeff[band][pop]=h*(coeff[band][pop]-random_soln)+coeff[band][pop]
        else:
          coeff[band][pop]=h*(coeff[band][pop]-global_best_order[band])+coeff[band][pop]
    for pop in range(population_size):
      for band in range(len(D)):
        int_pop[(pop*len(D))+band]=coeff[band][pop]

    # checking for a trap / conflict
    if copy_global_param!=global_best_order:
      copy_global_param=global_best_order
    else:
      count+=1 

    # changing int pop combination to exit local minima trap
    if count>=5:
      if q>np.random.rand(1)[0]:
        for pop in range(population_size):
          for band in range(len(D)):
            int_pop[(pop*len(D))+band]=int_pop[(pop*len(D))+band]+(w*(int_pop[(pop*len(D))+band]-global_best_order[band]))
      else:
        for section in range(k):
          for samp in range(n):
            for band in range(len(D)):
              int_pop[section*samp+band]=int_pop[section*samp+band]+(np.random.rand(1)[0] * (int_pop[section*samp+band]-local_best_order[section*len(D)+band]))
      w=w*np.random.rand(1)[0]    
    #print("BEst parameter so far = ",global_best_order)
    f.write("BEst parameter so far = "+' '.join(list(map(str,global_best_order)))+"\n")
    #print("Done.time elasped: " + str(time.time() - tic))
    f.write("Done.time elasped: "+str(time.time() - tic)+"\n")
  f.write("Total Execution Time: "+str(time.time()-ent_exc_time)+"\n")
  print("The final Best parameter : "+flag_entropy,global_best_order)
  f.close()
  return global_best_order

import random
def farm_land_fertility_compute_optim_PK(D,option,flag_entropy,output_file,max_iteration = 10):
  if option==0:
    print('=== PERMUTATION ENTROPY ===')
  else:
    print('=== KRASKOV ENTROPY ===')
  n=3  ### no of samples in a sector
  k=4  ### no of sections
  #max_iteration = 10
  population_size = 12  ### population size = n*k
  no_of_class = 2
  best_param=[]
  best_param_sof_far=[]
  int_pop = list(np.random.randint(0, 10, size =population_size*len(D)))
  kl_values = [-999999999]*(population_size*len(D))
  global_best_kl=[-999999999]*len(D)
  global_best_order=[-999999999]*len(D)
  local_best_kl=[-9999999999]*(len(D)*k)
  local_best_order=[-9999999999]*(len(D)*k)
  mean_kl=[-999999999]*(k*len(D))
  worst_section_kl=[999999999]*len(D)
  worst_section=[-1]*len(D)
  #external_memory_order=[0]*(population_size*len(D-1))
  alpha = np.random.rand(1)[0]
  count=0
  w=np.random.rand(1)[0]
  q=np.random.rand(1)[0]
  copy_global_param=global_best_order
  #copy_global_param=global_best_order
  f=open(output_file+'.txt','w')
  ent_exc_time=time.time()
  for iter in range(max_iteration):
    print("ITERATION ",iter," ------------------------------")
    tic = time.time() 
    
    #calculating kl divergence for population
    for pop in range(population_size):
      for band in range(len(D)): 
        if option==0:
          int_pop=[2 if i <2 else i for i in int_pop]
          int_pop=[15 if i >15 else i for i in int_pop]
          ent=permutation_ent(D[band],int_pop[pop*len(D)+band])
        else:
          int_pop=[1 if i <1 else i for i in int_pop]
          int_pop=[15 if i >15 else i for i in int_pop]
          ent=kraskov_ent(D[band],int_pop[pop*len(D)+band])
        a,e=np.split(ent,no_of_class)
        kl_values[pop*len(D)+band]=kl(a,e)

    #saving best kl and corresposnding order values
    for pop in range(population_size):
      for band in range(len(D)):
        if global_best_kl[band] < kl_values[(pop*5)+band]:
          global_best_kl[band]=kl_values[(pop*len(D))+band]
          global_best_order[band]=int_pop[(pop*len(D))+band]

    #saving local best kl and corresponding order
    for section in range(k):
      for samp in range(n):
        for band in range(len(D)):
          if local_best_kl[section*len(D)+band] < kl_values[section*samp+band]:
            local_best_kl[section*len(D)+band]=kl_values[section*samp+band]
            local_best_order[section*len(D)+band]=int_pop[section*samp+band]

    #calculating mean fitness
    len_sec=len(int_pop)//k
    for j in range(k):
      sliced_pop=kl_values[j*len_sec:(j*len_sec)+len_sec]
      x,y,z=sliced_pop[0:5],sliced_pop[5:10],sliced_pop[10:15]
      for band in range(len(D)):
        mean_kl[j*len(D)+band]=(x[band]+y[band]+z[band])/n
    
    #updating worst section kl
    for band in range(len(D)):
      for section in range(k):
        if mean_kl[section*len(D)+band] < worst_section_kl[band]:
          worst_section_kl[band]= mean_kl[section*len(D)+band]
          worst_section[band]=section
    
    
    # rearranging population into array (just for convenience)
    h=alpha*np.random.uniform(-1,1)
    coeff=np.array([[0]*population_size]*len(D))
    for pop in range(population_size):
      for band in range(len(D)): 
        coeff[band][pop]=int_pop[pop*len(D)+band]
    # udpdating population for next step
    for band in range(coeff.shape[0]):
      for pop in range(coeff.shape[1]):
        sec=pop//n
        if sec==worst_section[band]:
          l=list(coeff[band])
          del l[sec*n:(sec+1)*n]
          random_soln=random.choice(l)    ### selecting a random solution from external memory
          coeff[band][pop]=h*(coeff[band][pop]-random_soln)+coeff[band][pop]
        else:
          coeff[band][pop]=h*(coeff[band][pop]-global_best_order[band])+coeff[band][pop]
    for pop in range(population_size):
      for band in range(len(D)):
        int_pop[(pop*len(D))+band]=coeff[band][pop]

    # checking for a trap / conflict
    if copy_global_param!=global_best_order:
      copy_global_param=global_best_order
    else:
      count+=1 

    # changing int pop combination to exit local minima trap
    if count>=5:
      if q>np.random.rand(1)[0]:
        for pop in range(population_size):
          for band in range(len(D)):
            int_pop[(pop*len(D))+band]=int_pop[(pop*len(D))+band]+(w*(int_pop[(pop*len(D))+band]-global_best_order[band]))
      else:
        for section in range(k):
          for samp in range(n):
            for band in range(len(D)):
              int_pop[section*samp+band]=int_pop[section*samp+band]+(np.random.rand(1)[0] * (int_pop[section*samp+band]-local_best_order[section*len(D)+band]))
      w=w*np.random.rand(1)[0]    
    int_pop=list(np.round(int_pop).astype(int))   
    #print("BEst parameter so far = ",global_best_order)
    f.write("BEst parameter so far = "+' '.join(list(map(str,global_best_order)))+"\n")
    #print("Done.time elasped: " + str(time.time() - tic))
    f.write("Done.time elasped: "+str(time.time() - tic)+"\n")
  f.write("Total Execution Time: "+str(time.time()-ent_exc_time)+"\n")
  print("The final Best parameter : "+flag_entropy,global_best_order)
  f.close()
  return global_best_order

def farm_land_fertility_compute_optim_Approximation(D,output_file,m=2,flag_entropy='Approximation',max_iteration = 10):
  print('=== APPROXIMATION ENTROPY ===')
  n=3  ### no of samples in a sector
  k=4  ### no of sections
  
  population_size = 12  ### population size = n*k
  no_of_class = 2
  best_param=[]
  best_param_sof_far=[]
  #int_pop_m = list(np.random.randint(0, 10, size =population_size*len(D)))
  int_pop_r = list(np.random.uniform(0, 2, size =population_size*len(D)))
  kl_values = [-999999999]*(population_size*len(D))
  global_best_kl=[-999999999]*len(D)
  #global_best_m=[-999999999]*len(D)
  global_best_r=[-999999999]*len(D)
  local_best_kl=[-9999999999]*(len(D)*k)
  #local_best_m=[-9999999999]*(len(D)*k)
  local_best_r=[-9999999999]*(len(D)*k)
  mean_kl=[-999999999]*(k*len(D))
  worst_section_kl=[999999999]*len(D)
  worst_section=[-1]*len(D)
  #external_memory_order=[0]*(population_size*len(D-1))
  alpha = np.random.rand(1)[0]
  count=0
  w=np.random.rand(1)[0]
  q=np.random.rand(1)[0]
  #copy_global_param_m=global_best_m
  copy_global_r=global_best_r
  f=open(output_file+'.txt','w')
  ent_exc_time=time.time()
  for iter in range(max_iteration):
    #print("ITERATION ",iter," ------------------------------")
    f.write("ITERATION "+str(iter)+" ------------------------------ \n")
    tic = time.time() 
    
    #calculating kl divergence for population
    for pop in range(population_size):
      for band in range(len(D)): 
        int_pop_r=[0 if i <0 else i for i in int_pop_r]
        int_pop_r=[5 if i >5 else i for i in int_pop_r]
        ent=approx_entropy(D[band],m,int_pop_r[pop*len(D)+band])
      a,e=np.split(ent,no_of_class)
      kl_values[pop*len(D)+band]=kl(a,e)
    #saving best kl and corresposnding order values
    for pop in range(population_size):
      for band in range(len(D)):
        if global_best_kl[band] < kl_values[(pop*5)+band]:
          global_best_kl[band]=kl_values[(pop*len(D))+band]
          #global_best_m[band]=int_pop_m[(pop*len(D))+band]
          global_best_r[band]=int_pop_r[(pop*len(D))+band]

    #saving local best kl and corresponding order
    for section in range(k):
      for samp in range(n):
        for band in range(len(D)):
          if local_best_kl[section*len(D)+band] < kl_values[section*samp+band]:
            local_best_kl[section*len(D)+band]=kl_values[section*samp+band]
            #local_best_m[section*len(D)+band]=int_pop_m[section*samp+band]
            local_best_r[section*len(D)+band]=int_pop_r[section*samp+band]

    #calculating mean fitness
    len_sec=len(int_pop_r)//k
    for j in range(k):
      sliced_pop=kl_values[j*len_sec:(j*len_sec)+len_sec]
      x,y,z=sliced_pop[0:5],sliced_pop[5:10],sliced_pop[10:15]
      for band in range(len(D)):
        mean_kl[j*len(D)+band]=(x[band]+y[band]+z[band])/n
    
    #updating worst section kl
    for band in range(len(D)):
      for section in range(k):
        if mean_kl[section*len(D)+band] < worst_section_kl[band]:
          worst_section_kl[band]= mean_kl[section*len(D)+band]
          worst_section[band]=section
    
    
    # rearranging population into array (just for convenience)
    h=alpha*np.random.uniform(-1,1)
    #coeff_m=np.array([[0]*population_size]*len(D))
    coeff_r=np.array([[0]*population_size]*len(D))
    for pop in range(population_size):
      for band in range(len(D)): 
        #coeff_m[band][pop]=int_pop_m[pop*len(D)+band]
        coeff_r[band][pop]=int_pop_r[pop*len(D)+band]

    # udpdating population for next step
    for band in range(coeff_r.shape[0]):
      for pop in range(coeff_r.shape[1]):
        sec=pop//n
        if sec==worst_section[band]:
          #l_m=list(coeff_m[band])
          l_r=list(coeff_r[band])
          #del l_m[sec*n:(sec+1)*n]
          del l_r[sec*n:(sec+1)*n]
          #random_soln_m=random.choice(l_m)
          random_soln=random.choice(l_r)     ### selecting a random solution from external memory
          #coeff_m[band][pop]=h*(coeff_m[band][pop]-random_soln)+coeff_m[band][pop]
          coeff_r[band][pop]=h*(coeff_r[band][pop]-random_soln)+coeff_r[band][pop]
        else:
          #coeff_m[band][pop]=h*(coeff_m[band][pop]-global_best_m[band])+coeff_m[band][pop]
          coeff_r[band][pop]=h*(coeff_r[band][pop]-global_best_r[band])+coeff_r[band][pop]
    for pop in range(population_size):
      for band in range(len(D)):
        #int_pop_m[(pop*len(D))+band]=coeff_m[band][pop]
        int_pop_r[(pop*len(D))+band]=coeff_r[band][pop]

    # checking for a trap / conflict
    if copy_global_r!=global_best_r :
      #copy_global_m=global_best_m
      copy_global_r=global_best_r
    else:
      count+=1 

    # changing int pop combination to exit local minima trap
    if count>=5:
      if q>np.random.rand(1)[0]:
        for pop in range(population_size):
          for band in range(len(D)):
            #int_pop_m[(pop*len(D))+band]=int_pop_m[(pop*len(D))+band]+(w*(int_pop_m[(pop*len(D))+band]-global_best_m[band]))
            int_pop_r[(pop*len(D))+band]=int_pop_r[(pop*len(D))+band]+(w*(int_pop_r[(pop*len(D))+band]-global_best_r[band]))
      else:
        for section in range(k):
          for samp in range(n):
            for band in range(len(D)):
              #int_pop_m[section*samp+band]=int_pop_m[section*samp+band]+(np.random.rand(1)[0] * (int_pop_m[section*samp+band]-local_best_m[section*len(D)+band]))
              int_pop_r[section*samp+band]=int_pop_r[section*samp+band]+(np.random.rand(1)[0] * (int_pop_r[section*samp+band]-local_best_r[section*len(D)+band]))
      w=w*np.random.rand(1)[0]    
    #int_pop_m=list(np.round(int_pop_m).astype(int))
    print("BEst parameter so far = ",global_best_r)
    f.write("BEst parameter so far = "+' '.join(list(map(str,global_best_r))+"\n"))
    print("Done.time elasped: " + str(time.time() - tic))
    f.write("Done.time elasped: "+str(time.time() - tic)+"\n")
  print("The final Best parameter : "+flag_entropy,global_best_r)
  f.write("Total Execution Time: "+str(time.time()-ent_exc_time)+"\n")
  f.close()
  return (global_best_r)

#ss=farm_land_fertility_compute_optim_Approximation(D)
#ss.shape

#parallelize 
from multiprocessing import Pool
pool = Pool()

sns.set_theme(style='white')
def conf_mat(y_test,label,col,classifier):
  cm = confusion_matrix(y_test,label)
  df_cm = pd.DataFrame(cm, columns=col, index = col)
  df_cm.index.name = 'Actual'
  df_cm.columns.name = 'Predicted'
  plt.figure(figsize = (10,7))
  sns.set(font_scale=1.4)#for label size
  sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16},fmt='d').set_title("{} ({}) ".format(classifier,' - '.join(col)))

def classify_svm(X,Y,col):
    #X, Y = extract_best_features()
    acc = []
    classify = svm.SVC(gamma='scale', kernel = 'rbf')
    #ten_fold_score = cross_val_score(classify, X, Y, cv = 10)
    for _ in range(20):
        X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)
        classify.fit(X_train, Y_train)
        Y_pred = classify.predict(X_test)
        conf_mat(Y_test,Y_pred,col,"SVM")
        acc.append(accuracy_score(Y_test, Y_pred))
    #print(np.mean(np.array(acc)))
    np.save("spl_param_svm_rbf", acc)
    return np.mean(np.array(acc))

def classify_LDA(X,Y,col):
    #X, Y = extract_best_features()
    acc = []
    classify = LinearDiscriminantAnalysis()
    #ten_fold_score = cross_val_score(classify, X, Y, cv = 10)
    for _ in range(20):
        X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)
        classify.fit(X_train, Y_train)
        Y_pred = classify.predict(X_test)
        conf_mat(Y_test,Y_pred,col,"LDA")
        acc.append(accuracy_score(Y_test, Y_pred))
    #print(np.mean(np.array(acc)))
    np.save("spl_param_LDA", acc)
    return np.mean(np.array(acc))

def classify_KNN(X,Y,col):
    #X, Y = extract_best_features()
    acc = []
    classify = KNeighborsClassifier(n_neighbors=3)
    #ten_fold_score = cross_val_score(classify, X, Y, cv = 10)
    for _ in range(20):
        X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)
        classify.fit(X_train, Y_train)
        Y_pred = classify.predict(X_test)
        conf_mat(Y_test,Y_pred,col,"KNN")
        acc.append(accuracy_score(Y_test, Y_pred))
    #print(np.mean(np.array(acc)))
    np.save("spl_param_KNN", acc)
    return np.mean(np.array(acc))

def classify_RF(X,Y,col):
    #X, Y = extract_best_features()
    acc = []
    classify = RandomForestClassifier(max_depth=2, random_state=0)
    #ten_fold_score = cross_val_score(classify, X, Y, cv = 10)
    for _ in range(20):
        X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)
        classify.fit(X_train, Y_train)
        Y_pred = classify.predict(X_test)
        conf_mat(Y_test,Y_pred,col,"RF")
        acc.append(accuracy_score(Y_test, Y_pred))
    #print(np.mean(np.array(acc)))
    np.save("spl_param_RF", acc)
    return np.mean(np.array(acc))

def classify_ANN(X,Y,col):
    #X, Y = extract_best_features()
    acc = []
    
    #ten_fold_score = cross_val_score(classify, X, Y, cv = 10)
    for _ in range(20):
        model = Sequential()
        model.add(Dense(20, input_dim=X.shape[1],kernel_initializer='normal', activation='relu'))
        model.add(Dense(32,kernel_initializer='normal', activation='relu'))
        model.add(Dense(16,kernel_initializer='normal', activation='relu'))
        model.add(Dense(1, activation='sigmoid'))
        # Compile model
        model.compile(loss='binary_crossentropy', optimizer='adam')
        # fit the keras model on the dataset

        X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)
        model.fit(np.array(X_train), np.array(Y_train), epochs=50, batch_size=5, verbose=0)
        Y_pred = model.predict(np.array(X_test))
        conf_mat(np.array(Y_test),np.round(Y_pred),col,"ANN")
        acc.append(accuracy_score(np.array(Y_test), np.round(Y_pred)))
    #print(np.mean(np.array(acc)))
    np.save("spl_param_ANN", acc)
    return np.mean(np.array(acc))

# class A vs class E 
tic = time.time() 
new_File=open('Best_Parameters_A_E.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_A_E'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_A_E'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_A_E'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_A_E']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()

new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
col=['A','E']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

X.shape

X.shape,X[5]

answer3,permutation_ent(D[0],answer3[1])

D[0].shape

tic = time.time() 
new_File=open('Default_Best_Parameters_A_E.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
col=['A','E']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)

new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

print(str(SVM_acc),LDA_acc,KNN_acc,RF_acc,ANN_acc)

print(X)

# class C vs class D 
data=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Best_Parameters_C_D.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_C_D'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_C_D'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_C_D'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_C_D']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()
new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
col=['C','D']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class C vs class D 
data=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
tic = time.time() 
new_File=open('Default_Best_Parameters_C_D.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
  
col=['C','D']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class C vs class E
data=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Best_Parameters_C_E.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_C_E'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_C_E'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_C_E'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_C_E']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()
new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['C','E']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class C vs class E
data=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Default_Best_Parameters_C_E.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['C','E']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class D vs class E
data=[]
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Best_Parameters_D_E.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_D_E'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_D_E'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_D_E'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_D_E']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()
new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['D','E']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class D vs class E
data=[]
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Default_Best_Parameters_D_E.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['D','E']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class A vs class B
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Best_Parameters_A_B.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_A_B'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_A_B'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_A_B'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_A_B']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()
new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['A','B']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
tic = time.time() 
new_File=open('Default_Best_Parameters_A_B.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['A','B']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class A vs class C
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Best_Parameters_A_C.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_A_C'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_A_C'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_A_C'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_A_C']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()
new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['A','C']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class A vs class C
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
tic = time.time() 
new_File=open('Default_Best_Parameters_A_C.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['A','C']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class B vs class C
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Best_Parameters_B_C.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_B_C'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_B_C'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_B_C'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_B_C']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()
new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['B','C']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class B vs class C
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Default_Best_Parameters_B_C.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
col=['B','C']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class A vs class D
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Best_Parameters_A_D.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_A_D'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_A_D'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_A_D'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_A_D']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()
new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['A','D']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)

new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class A vs class D
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Default_Best_Parameters_A_D.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['A','D']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class B vs class D
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Best_Parameters_B_D.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_B_D'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_B_D'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_B_D'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_B_D']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()
new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['B','D']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class B vs class D
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Default_Best_Parameters_B_D.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['B','D']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class B vs class E
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Best_Parameters_B_E.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_B_E'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_B_E'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_B_E'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_B_E']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()
new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['B','E']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class B vs class E
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Default_Best_Parameters_B_E.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['B','E']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class CD vs class E
data=[]
for i in range(len(c)//2):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

for i in range(len(d)//2):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Best_Parameters_CD_E.txt','w')
result1 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,0,'Reyni','Reyni_CD_E'])    # evaluate "solve1(A)" asynchronously
result2 = pool.apply_async(farm_land_fertility_compute_optim_RT, [D,1,'Tsallis','Tsallis_CD_E'])
result3 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,0,'Permutation','Permutation_CD_E'])    # evaluate "solve1(A)" asynchronously
result4 = pool.apply_async(farm_land_fertility_compute_optim_PK, [D,1,'Kraskov','Kraskov_CD_E']) 
#result5 = pool.apply_async(farm_land_fertility_compute_optim_Approximation, [D])     # evaluate "solve2(B)" asynchronously
answer1 = result1.get()
answer2 = result2.get()
answer3=  result3.get()
answer4= result4.get()
#answer5= result5.get()
new_File.write('Reyni Entropy : '+' '.join(map(str,answer1))+"\n")
new_File.write('Tsallis Entropy : '+' '.join(map(str,answer2))+"\n")
new_File.write('Permutation Entropy : '+' '.join(map(str,answer3))+"\n")
new_File.write('Kraskov Entropy : '+' '.join(map(str,answer4))+"\n")
#new_File.write(' '.join(map(str,answer5))+"\n")
new_File.write("Done.time elasped: " + str(time.time() - tic))
print("Done.time elasped: " + str(time.time() - tic))
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['CD','E']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

# class CD vs class E
data=[]
for i in range(len(c)//2):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

for i in range(len(d)//2):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

tic = time.time() 
new_File=open('Default_Best_Parameters_CD_E.txt','w')
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)

col=['CD','E']
SVM_acc=classify_svm(X,y,col)
LDA_acc=classify_LDA(X,y,col)
KNN_acc=classify_KNN(X,y,col)
RF_acc=classify_RF(X,y,col)
ANN_acc=classify_ANN(X,y,col)
###
new_File.write('SVM : '+''.join(map(str,str(SVM_acc)))+"\n")
new_File.write('LDA : '+''.join(map(str,str(LDA_acc)))+"\n")
new_File.write('KNN : '+''.join(map(str,str(KNN_acc)))+"\n")
new_File.write('RF : '+''.join(map(str,str(RF_acc)))+"\n")
new_File.write('ANN : '+''.join(map(str,str(ANN_acc)))+"\n")
print(SVM_acc,LDA_acc,KNN_acc,RF_acc,ANN_acc)
new_File.close()
print("Done.time elasped: " + str(time.time() - tic))

#ls

import pandas as pd

reyni_a4=[1.953268978,2.466530204,1.947881774,1.521442639,2.382846527,2.469540351,1.546540681,1.823905461,2.407300969,2.266618553,1.953461716]
reyni_d4=[2.026189876,1.824118746,2.123950856,2.15500126,1.970800463,1.69048893,2.409236084,1.514085684,2.24499672,2.217126826,1.8]
reyni_d3=[1.698367013,1.589902261,2.14315706,1.791686377,2.216194884,1.857186535,1.863587235,2.111308196,1.775638402,2.466164551,2.138596882]
reyni_d2=[1.683903337,2.338981628,1.8,2.065445124,1.882858544,1.675386165,2.244758676,2.027067018,2,1.606241714,2.06257209]
reyni_d1=[2.276179193,1.955450125,2,1.512713597,1.5,2.180150577,1.994110366,1.719671276,2.18795545,1.86197088,2.03964365]

tsallis_a4=[2.462496839,2.471614764,2.195420683,2.409624865,2.382846527,2.469540351,2.463438901,2.5,2.464649644,2.335507723,2.1957008482567493]
tsallis_d4=[2.47623377,2.368192071,2.197723784,2.496818465,2.413390239,2.457143684,2.429253538,2.39832991,2.43293504,2.397384721,2.155124442]
tsallis_d3=[2.377101415,2.495495289,2.163408631,2.254386798,2.477570864,2.370024795,2.324362984,2.469873703,2.407264083,2.466164551,2.138596882]
tsallis_d2=[2.230365419,2.475446525,2.188955935,2.304811706,2.444184487,2.413506497,2.442048241,2.331340331,2.34444575,2.480924379,2.161951002]
tsallis_d1=[2.409643995,2.466585919,2.189809728,2.317794518,2.453111475,2.369028209,2.431554863,2.392965253,2.18795545,2.384046975,2.162500931]

permutation_a4=[5,5,5,5,5,5,5,5,6,5,6]
permutation_d4=[5,5,5,5,5,5,5,5,5,5,5]
permutation_d3=[5,6,5,5,5,5,5,5,5,5,5]
permutation_d2=[6,6,6,6,6,6,6,6,6,6,6]
permutation_d1=[6,6,7,6,6,6,6,6,6,6,6]

kraskov_a4=[4,2,2,2,2,3,2,4,3,4,2]
kraskov_d4=[2,5,6,5,6,4,3,6,4,2,2]
kraskov_d3=[2,4,7,5,6,8,8,9,8,3,5]
kraskov_d2=[9,9,7,7,5,9,13,9,11,9,7]
kraskov_d1=[8,15,15,8,15,9,9,15,9,9,15]

SVM_BA=[]
LDA_BA=[]
KNN_BA=[]
RF_BA=[]
ANN_BA=[]
classes=['A-E', 'C-D','C/D-E','C-E','D-E','A-B','A-C','B-C','A-D','B-D','B-E']
cm_results_best={'binary_classification' :classes ,'TP':[],'TN':[],'FP':[],'FN':[]}
# class A vs class E ---- Best Paramter
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[0],reyni_d4[0],reyni_d3[0],reyni_d2[0],reyni_d1[0]]
answer2=[tsallis_a4[0],tsallis_d4[0],tsallis_d3[0],tsallis_d2[0],tsallis_d1[0]]
answer3=[permutation_a4[0],permutation_d4[0],permutation_d3[0],permutation_d2[0],permutation_d1[0]]
answer4=[kraskov_a4[0],kraskov_d4[0],kraskov_d3[0],kraskov_d2[0],kraskov_d1[0]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))
#----------------------------------------------------------------------------------

# class C vs class D ---- Best Paramter
data=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[1],reyni_d4[1],reyni_d3[1],reyni_d2[1],reyni_d1[1]]
answer2=[tsallis_a4[1],tsallis_d4[1],tsallis_d3[1],tsallis_d2[1],tsallis_d1[1]]
answer3=[permutation_a4[1],permutation_d4[1],permutation_d3[1],permutation_d2[1],permutation_d1[1]]
answer4=[kraskov_a4[1],kraskov_d4[1],kraskov_d3[1],kraskov_d2[1],kraskov_d1[1]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))
#-------------------------------------------------------------------------------

# class C/D vs class E ---- Best Paramter
data=[]
for i in range(len(c)//2):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

for i in range(len(d)//2):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[2],reyni_d4[2],reyni_d3[2],reyni_d2[2],reyni_d1[2]]
answer2=[tsallis_a4[2],tsallis_d4[2],tsallis_d3[2],tsallis_d2[2],tsallis_d1[2]]
answer3=[permutation_a4[2],permutation_d4[2],permutation_d3[2],permutation_d2[2],permutation_d1[2]]
answer4=[kraskov_a4[2],kraskov_d4[2],kraskov_d3[2],kraskov_d2[2],kraskov_d1[2]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))
#-------------------------------------------------------------------------------

# class C vs class E ---- Best Paramter
data=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[3],reyni_d4[3],reyni_d3[3],reyni_d2[3],reyni_d1[3]]
answer2=[tsallis_a4[3],tsallis_d4[3],tsallis_d3[3],tsallis_d2[3],tsallis_d1[3]]
answer3=[permutation_a4[3],permutation_d4[3],permutation_d3[3],permutation_d2[3],permutation_d1[3]]
answer4=[kraskov_a4[3],kraskov_d4[3],kraskov_d3[3],kraskov_d2[3],kraskov_d1[3]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))
#-------------------------------------------------------------------------------


# class D vs class E ---- Best Paramter
data=[]
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[4],reyni_d4[4],reyni_d3[4],reyni_d2[4],reyni_d1[4]]
answer2=[tsallis_a4[4],tsallis_d4[4],tsallis_d3[4],tsallis_d2[4],tsallis_d1[4]]
answer3=[permutation_a4[4],permutation_d4[4],permutation_d3[4],permutation_d2[4],permutation_d1[4]]
answer4=[kraskov_a4[4],kraskov_d4[4],kraskov_d3[4],kraskov_d2[4],kraskov_d1[4]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))
#------------------------------------------------------------------------------

# class A vs class B ---- Best Paramter
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[5],reyni_d4[5],reyni_d3[5],reyni_d2[5],reyni_d1[5]]
answer2=[tsallis_a4[5],tsallis_d4[5],tsallis_d3[5],tsallis_d2[5],tsallis_d1[5]]
answer3=[permutation_a4[5],permutation_d4[5],permutation_d3[5],permutation_d2[5],permutation_d1[5]]
answer4=[kraskov_a4[5],kraskov_d4[5],kraskov_d3[5],kraskov_d2[5],kraskov_d1[5]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))
#------------------------------------------------------------------------------

# class A vs class C ---- Best Paramter
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[6],reyni_d4[6],reyni_d3[6],reyni_d2[6],reyni_d1[6]]
answer2=[tsallis_a4[6],tsallis_d4[6],tsallis_d3[6],tsallis_d2[6],tsallis_d1[6]]
answer3=[permutation_a4[6],permutation_d4[6],permutation_d3[6],permutation_d2[6],permutation_d1[6]]
answer4=[kraskov_a4[6],kraskov_d4[6],kraskov_d3[6],kraskov_d2[6],kraskov_d1[6]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))
#----------------------------------------------------------------------------

# class B vs class C ---- Best Paramter
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[7],reyni_d4[7],reyni_d3[7],reyni_d2[7],reyni_d1[7]]
answer2=[tsallis_a4[7],tsallis_d4[7],tsallis_d3[7],tsallis_d2[7],tsallis_d1[7]]
answer3=[permutation_a4[7],permutation_d4[7],permutation_d3[7],permutation_d2[7],permutation_d1[7]]
answer4=[kraskov_a4[7],kraskov_d4[7],kraskov_d3[7],kraskov_d2[7],kraskov_d1[7]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))
#----------------------------------------------------------------------------

# class A vs class D ---- Best Paramter
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[8],reyni_d4[8],reyni_d3[8],reyni_d2[8],reyni_d1[8]]
answer2=[tsallis_a4[8],tsallis_d4[8],tsallis_d3[8],tsallis_d2[8],tsallis_d1[8]]
answer3=[permutation_a4[8],permutation_d4[8],permutation_d3[8],permutation_d2[8],permutation_d1[8]]
answer4=[kraskov_a4[8],kraskov_d4[8],kraskov_d3[8],kraskov_d2[8],kraskov_d1[8]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))
#----------------------------------------------------------------------------

# class B vs class D ---- Best Paramter
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[9],reyni_d4[9],reyni_d3[9],reyni_d2[9],reyni_d1[9]]
answer2=[tsallis_a4[9],tsallis_d4[9],tsallis_d3[9],tsallis_d2[9],tsallis_d1[9]]
answer3=[permutation_a4[9],permutation_d4[9],permutation_d3[9],permutation_d2[9],permutation_d1[9]]
answer4=[kraskov_a4[9],kraskov_d4[9],kraskov_d3[9],kraskov_d2[9],kraskov_d1[9]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))

#----------------------------------------------------------------------------

# class B vs class E ---- Best Paramter
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]
answer1=[reyni_a4[10],reyni_d4[10],reyni_d3[10],reyni_d2[10],reyni_d1[10]]
answer2=[tsallis_a4[10],tsallis_d4[10],tsallis_d3[10],tsallis_d2[10],tsallis_d1[10]]
answer3=[permutation_a4[10],permutation_d4[10],permutation_d3[10],permutation_d2[10],permutation_d1[10]]
answer4=[kraskov_a4[10],kraskov_d4[10],kraskov_d3[10],kraskov_d2[10],kraskov_d1[10]]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_BA.append(classify_svm(X,y))
LDA_BA.append(classify_LDA(X,y))
KNN_BA.append(classify_KNN(X,y))
RF_BA.append(classify_RF(X,y))
ANN_BA.append(classify_ANN(X,y))

SVM_DA=[]
LDA_DA=[]
KNN_DA=[]
RF_DA=[]
ANN_DA=[]
classes=['A-E', 'C-D','C/D-E','C-E','D-E','A-B','A-C','B-C','A-D','B-D','B-E']

# class A vs class E ---- Default Paramter
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))
#----------------------------------------------------------------------------------

# class C vs class D ---- Default Paramter
data=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))
#-------------------------------------------------------------------------------

# class C/D vs class E ---- Default Paramter
data=[]
for i in range(len(c)//2):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

for i in range(len(d)//2):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))
#-------------------------------------------------------------------------------

# class C vs class E ---- Default Paramter
data=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))
#-------------------------------------------------------------------------------


# class D vs class E ---- Default Paramter
data=[]
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))
#------------------------------------------------------------------------------

# class A vs class B ---- Default Paramter
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))
#------------------------------------------------------------------------------

# class A vs class C ---- Default Paramter
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))
#----------------------------------------------------------------------------

# class B vs class C ---- Default Paramter
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))
#----------------------------------------------------------------------------

# class A vs class D ---- Default Paramter
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))
#----------------------------------------------------------------------------

# class B vs class D ---- Default Paramter
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))

#----------------------------------------------------------------------------

# class B vs class E ---- Default Paramter
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
l1=[]
l2=[]
l3=[]
l4=[]

for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 
print(len(l1),len(l2),len(l3),len(l4))

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)

scaler = MinMaxScaler()
X=scaler.fit_transform(X)
'''
for i in range(len(X)):
  X[i]= normalize(X[i,np.newaxis], axis=0).ravel()'''
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_DA.append(classify_svm(X,y))
LDA_DA.append(classify_LDA(X,y))
KNN_DA.append(classify_KNN(X,y))
RF_DA.append(classify_RF(X,y))
ANN_DA.append(classify_ANN(X,y))

plt.figure(figsize=(20,10))
plt.title('SVM (RBF KERNEL) ')

plt.xlabel('Classes')
plt.ylabel('accuracy')
plt.plot(classes,SVM_BA,'ro')
plt.plot(classes,SVM_DA,'go')
plt.legend(['Best Parameter','Default Parameter'],loc='lower right')
#plt.show()

plt.figure(figsize=(20,10))
plt.title('LDA')

plt.xlabel('Classes')
plt.ylabel('accuracy')
plt.plot(classes,LDA_BA,'ro')
plt.plot(classes,LDA_DA,'go')
plt.legend(['Best Parameter','Default Parameter'],loc='lower right')
#plt.show()

plt.figure(figsize=(20,10))
plt.title('KNN')

plt.xlabel('Classes')
plt.ylabel('accuracy')
plt.plot(classes,KNN_BA,'ro')
plt.plot(classes,KNN_DA,'go')
plt.legend(['Best Parameter','Default Parameter'],loc='lower right')
#plt.show()

plt.figure(figsize=(20,10))
plt.title('RF ')

plt.xlabel('Classes')
plt.ylabel('accuracy')
plt.plot(classes,RF_BA,'ro')
plt.plot(classes,RF_DA,'go')
plt.legend(['Best Parameter','Default Parameter'],loc='lower right')
#plt.show()

plt.figure(figsize=(20,10))
plt.title('ANN ')

plt.xlabel('Classes')
plt.ylabel('accuracy')
plt.plot(classes,ANN_BA,'ro')
plt.plot(classes,ANN_DA,'go')
plt.legend(['Best Parameter','Default Parameter'],loc='lower right')
#plt.show()

SVM_best=[0.993333333,0.585,0.975,0.983333333,0.983333333,0.905,0.938333333,0.975,0.976666667,0.99,0.97]
LDA_best=[1,0.663333333,0.975,0.981666667,0.986666667,0.96,0.988333333,0.991666667,0.981666667,0.995,1]
kNN_best=[0.991666667,0.691666667,0.991666667,0.981666667,0.985,0.908333333,0.991666667,0.99,0.981966667,0.988333333,0.986666667 ]
RF_best=[0.995,0.663333333,0.98,0.986666667,0.97,0.918333333,0.956666667,0.978333333,0.928333333,0.983333333,0.988333333]
ANN_best=[0.936666667,0.583333333,0.983333333,0.986666667,0.975,0.903333333,0.928333333,0.981666667,0.915,0.983333333,0.948333333]

SVM_default=[0.988333333,0.53,0.971666667,0.9799,0.975,0.895,0.926666667,0.968333333,0.963333333,0.98833,0.955]
LDA_default=[0.991666667,0.628333333,0.97,0.97499,0.965,0.926666667,0.963333333,0.988333333,0.978333333,0.996,1]
kNN_default=[0.991,0.63,0.976666667,0.985,0.983333333,0.901666667,0.988333333,0.989,0.981,0.99,0.981666]
RF_default=[0.99,0.618333333,0.968333333,0.976666667,0.963333333,0.878333333,0.936666667,0.971666667,0.905,0.965,0.988333333]
ANN_default=[0.92,0.54,0.97,0.95833333,0.96,0.89,0.908333333,0.976666667,0.903333333,0.975,0.8999]

classes=['A-E', 'C-D','C/D-E','C-E','D-E','A-B','A-C','B-C','A-D','B-D','B-E']

plt.figure(figsize=(20,10))
plt.title('SVM (RBF KERNEL) ')

plt.xlabel('Classes')
plt.ylabel('accuracy')
plt.plot(classes,SVM_best,'ro')
plt.plot(classes,SVM_default,'go')
plt.legend(['Best Parameter','Default Parameter'],loc='lower right')
#plt.show()

plt.figure(figsize=(20,10))
plt.title(' LDA ')

plt.xlabel('Classes')
plt.ylabel('accuracy')
plt.plot(classes,LDA_best,'ro')
plt.plot(classes,LDA_default,'go')
plt.legend(['Best Parameter','Default Parameter'],loc='lower right')
#plt.show()

plt.figure(figsize=(20,10))
plt.title('kNN ')

plt.xlabel('Classes')
plt.ylabel('accuracy')
plt.plot(classes,kNN_best,'ro')
plt.plot(classes,kNN_default,'go')
plt.legend(['Best Parameter','Default Parameter'],loc='lower right')
#plt.show()

plt.figure(figsize=(20,10))
plt.title('Random Forest ')

plt.xlabel('Classes')
plt.ylabel('accuracy')
plt.plot(classes,RF_best,'ro')
plt.plot(classes,RF_default,'go')
plt.legend(['Best Parameter','Default Parameter'],loc='lower right')
#plt.show()

plt.figure(figsize=(20,10))
plt.title(' ANN ')

plt.xlabel('Classes')
plt.ylabel('accuracy')
plt.plot(classes,ANN_best,'ro')
plt.plot(classes,ANN_default,'go')
plt.legend(['Best Parameter','Default Parameter'],loc='lower right')
#plt.show()



"""BAR PLOTS"""

import matplotlib.pyplot as plt

classes=['A-E','C-D','C/D-E','C-E','D-E','A-B','A-C','B-C','A-D','B-D','B-E']

SVM_D=[0.991666667,0.713333333,0.97,0.983333333,0.971666667,0.931666667,0.986666667,0.988333333,0.983333333,0.998333333,1]
SVM_B=[0.998333333,0.718333333,0.98,0.99,0.976666667,0.941666667,0.993333333,0.991666667,0.99,1,1]

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(SVM_D) / 8, step=0.125)

low = min(SVM_D)
high = max(SVM_B)

plt.bar(x_range, SVM_D, color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, SVM_B, color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(SVM_B):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('SVM (RBF Kernel) ')
#plt.xlabel('classes')
plt.ylabel('accuracy')
plt.show()

"""LDA"""

LDA_D=[0.991666667,0.64,0.97,0.976666667,0.97,0.933333333,0.951666667,0.986666667,0.975,0.995,1]
LDA_B=[0.995,0.658333333,0.983333333,0.983333333,0.983333333,0.946666667,0.99,0.998333333,0.993333333,0.996666667,1]
barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(LDA_D) / 8, step=0.125)


plt.bar(x_range, LDA_D, color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, LDA_B, color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(LDA_B):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('LDA ')
#plt.xlabel('classes')
plt.ylabel('accuracy')
plt.show()

"""KNN"""

KNN_D=[0.994,0.678333333,0.976666667,0.976666667,0.975,0.916666667,0.965,0.983333333,0.968333333,1,0.99]
KNN_B=[1,0.785,0.983333333,0.995,0.983333333,0.935,0.99,0.998333333,0.998333333,1,0.998333333]
barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(KNN_D) / 8, step=0.125)


plt.bar(x_range, KNN_D, color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, KNN_B, color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(KNN_B):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('KNN ')
#plt.xlabel('classes')
plt.ylabel('accuracy')
plt.show()

"""RF"""

RF_D=[0.99,0.633333333,0.965,0.976666667,0.968333333,0.905,0.935,0.955,0.903333333,0.961666667,0.985]
RF_B=[0.996666667,0.65,0.975,0.981666667,0.973333333,0.93,0.968333333,0.983333333,0.918333333,0.98,0.998333333]
barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(RF_D) / 8, step=0.125)


plt.bar(x_range, RF_D, color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, RF_B, color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(RF_B):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('Random Forest ')
#plt.xlabel('classes')
plt.ylabel('accuracy')
plt.show()

"""ANN"""

ANN_D=[0.994,0.631666667,0.978333333,0.98,0.971666667,0.936666667,0.941666667,0.971666667,0.963333333,0.988333333,0.996666667]
ANN_B=[0.998333333,0.658333333,0.98,0.983333333,0.985,0.948333333,0.976666667,0.99,0.986666667,1,1]
barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(ANN_D) / 8, step=0.125)


plt.bar(x_range, ANN_D, color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, ANN_B, color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(ANN_B):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('ANN ')
#plt.xlabel('classes')
plt.ylabel('accuracy')
plt.show()

"""SENSITIVITY AND SPECIFICITY

-------------------------------------------------------------------------
"""

from sklearn.metrics import confusion_matrix
from sklearn import svm
def classify_svm(X,Y):
    #X, Y = extract_best_features()
    acc = []
    classify = svm.SVC(gamma='scale', kernel = 'rbf')
    sensitivity , specificity =0,0
    X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)
    classify.fit(X_train, Y_train)
    Y_pred = classify.predict(X_test)
    cm = confusion_matrix(Y_test,Y_pred)
    total=sum(sum(cm))
    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])
    specificity = cm[1,1]/(cm[1,0]+cm[1,1])
    TP=cm[0,0]
    TN=cm[1,1]
    FP=cm[1,0]
    FN=cm[0,1]
    #print(np.mean(np.array(acc)))
    return [sensitivity,specificity,TP,TN,FP,FN]

def classify_LDA(X,Y):
    #X, Y = extract_best_features()
    acc = []
    classify = LinearDiscriminantAnalysis()
    sensitivity , specificity =0,0
    X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)
    classify.fit(X_train, Y_train)
    Y_pred = classify.predict(X_test)
    cm = confusion_matrix(Y_test,Y_pred)
    total=sum(sum(cm))
    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])
    specificity = cm[1,1]/(cm[1,0]+cm[1,1])
    TP=cm[0,0]
    TN=cm[1,1]
    FP=cm[1,0]
    FN=cm[0,1]
    #print(np.mean(np.array(acc)))
    return [sensitivity,specificity,TP,TN,FP,FN]

def classify_KNN(X,Y):
    #X, Y = extract_best_features()
    acc = []
    classify = KNeighborsClassifier(n_neighbors=3)
    sensitivity , specificity =0,0
    X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)
    classify.fit(X_train, Y_train)
    Y_pred = classify.predict(X_test)
    cm = confusion_matrix(Y_test,Y_pred)
    total=sum(sum(cm))
    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])
    specificity = cm[1,1]/(cm[1,0]+cm[1,1])
    TP=cm[0,0]
    TN=cm[1,1]
    FP=cm[1,0]
    FN=cm[0,1]
    #print(np.mean(np.array(acc)))
    return [sensitivity,specificity,TP,TN,FP,FN]

def classify_RF(X,Y):
    #X, Y = extract_best_features()
    acc = []
    classify = RandomForestClassifier(max_depth=2, random_state=0)
    sensitivity , specificity =0,0
    X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)
    classify.fit(X_train, Y_train)
    Y_pred = classify.predict(X_test)
    cm = confusion_matrix(Y_test,Y_pred)
    total=sum(sum(cm))
    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])
    specificity = cm[1,1]/(cm[1,0]+cm[1,1])
    TP=cm[0,0]
    TN=cm[1,1]
    FP=cm[1,0]
    FN=cm[0,1]
    #print(np.mean(np.array(acc)))
    return [sensitivity,specificity,TP,TN,FP,FN]

def classify_ANN(X,Y):
    #X, Y = extract_best_features()
    acc = []
    sensitivity , specificity =0,0
    #ten_fold_score = cross_val_score(classify, X, Y, cv = 10)
    model = Sequential()
    model.add(Dense(20, input_dim=X.shape[1],kernel_initializer='normal', activation='relu'))
    model.add(Dense(32,kernel_initializer='normal', activation='relu'))
    model.add(Dense(16,kernel_initializer='normal', activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam')
    # fit the keras model on the dataset

    X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)
    model.fit(np.array(X_train), np.array(Y_train), epochs=50, batch_size=5, verbose=0)
    Y_pred = model.predict(np.array(X_test))
        
    cm = confusion_matrix(np.array(Y_test), np.round(Y_pred))
    total=sum(sum(cm))
    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])
    specificity = cm[1,1]/(cm[1,0]+cm[1,1])
    TP=cm[0,0]
    TN=cm[1,1]
    FP=cm[1,0]
    FN=cm[0,1]
    #print(np.mean(np.array(acc)))
    return [sensitivity,specificity,TP,TN,FP,FN]

Sn=[]
Sp=[]
Sn_def=[]
Sp_def=[]

TP=[]
TN=[]
FP=[]
FN=[]

TP_def=[]
TN_def=[]
FP_def=[]
FN_def=[]

# class A vs class E
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)

answer1=[2.115935533,1.943892514,2.048150869,2.173403917,2.147304864	]
answer2=[2.185943045,2.17748821,2.150594484,2.175323793,2.147304864	]
answer3=[	5,5,5,6,6]
answer4=[2,5,7,4,15]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

# class C vs class D 
data=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
		

answer1=[2,1.933417092,2.113264573,2.038770224,1.938885333	]
answer2=[2.186955747,2.139987569,2.186441231,2.178334076,2.14882106]
answer3=[5,5,5,6,6]
answer4=[2,4,5,15,15]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

# class CD vs class E 
data=[]
for i in range(len(c)//2):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(d)//2):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
				

answer1=[2.089952688,2.0171004,2.186624404,2.011651726,2.058031153]
answer2=[2.168557855,2.187271231,2.188694445,2.193917877,2.195208443]
answer3=[5,5,5,6,6]
answer4=[3,2,9,4,15]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

# class C vs class E
data=[]
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
		

answer1=[2.154843458,2.08125021,2.037455305,2.097644185,1.977343606	]
answer2=[2.16426674,2.172322717,2.163345567,2.127012812,2.129274956	]
answer3=[5,5,5,6,6]
answer4=[4,6,9,8,15]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

# class D vs class E
data=[]
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)
for i in range(len(e)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
		

answer1=[1.997546675,1.867524337,2.07919604,2.015235262,2.18331959	]
answer2=[2.198570373,2.186927073,2.170354079,2.169910325,2.19303475]
answer3=[5,5,5,6,6]
answer4=[4,4,6,8,15]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

# class A vs class B
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
			

answer1=[1.914901894,2.086622247,2.101411766,1.895496317,2.023003584]
answer2=[	2.193270213,2.134762226,2.185504762,2.185407508,2.194939792]
answer3=[5,5,5,6,6]
answer4=[3,4,3,8,9]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

# class A vs class C
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)


answer1=[1.979934112,2.099062624,2,1.863386362,2.060987297	]
answer2=[2.188729638,2.175838891,2.193616209,2.194343626,2.168119007	]
answer3=[5,5,5,6,6]
answer4=[	2,3,9,9,11]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

# class B vs class C
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(c)):
  temp=[]
  for j in range(len(c[i])):
    if(c[i][j]!=''):
      temp.append(int(c[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)


answer1=[2.060376838,1.962049954,2.033211687,1.941343294,1.922059743]
answer2=[2.144830084,2.169543553,2.193583456,2.190371331,2.171371992]
answer3=[6,4,4,6,5]
answer4=[2,7,7,9,9]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

# class A vs class D
data=[]
for i in range(len(a)):
  temp=[]
  for j in range(len(a[i])):
    if(a[i][j]!=''):
      temp.append(int(a[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
	

answer1=[2.012286147,1.986135723,1.88279998,1.927349177,2.095860404]
answer2=[2.187322519,2.199129429,2.187161494,2.150655251,2.190764735]
answer3=[5,5,4,6,6]
answer4=[2,5,8,15,9]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

# class B vs class D
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(d[i])):
    if(d[i][j]!=''):
      temp.append(int(d[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)


answer1=[2,2.071696926,2.115927541,2.098267402,2.102001457]
answer2=[2.188334142,2.188547696,2.139299652,2.137335008,2.175543818]
answer3=[6,5,5,6,6]
answer4=[4,5,9,15,9]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

# class B vs class E
data=[]
for i in range(len(b)):
  temp=[]
  for j in range(len(b[i])):
    if(b[i][j]!=''):
      temp.append(int(b[i][j]))
  data.append(temp)
for i in range(len(d)):
  temp=[]
  for j in range(len(e[i])):
    if(e[i][j]!=''):
      temp.append(int(e[i][j]))
  data.append(temp)

data=np.array(data)
D=bonn_dataset(data)
				

answer1=[2.094751717,2.045605663,1.92098413,2.001380296,2.195208559	]
answer2=[2.177908948,2.050143585,2.124629241,2.169807864,2.195208559	]
answer3=[5,5,5,6,6]
answer4=[3,3,8,8,9]
l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],answer1[band]))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],answer2[band]))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],answer3[band]))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],answer4[band])) 


X_ff=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X_ff=scaler.fit_transform(X_ff)
y_ff=[]
for i in range(0,100):
  y_ff.append(0)
for i in range(0,100):
  y_ff.append(1)
SVM=classify_svm(X_ff,y_ff)
LDA=classify_LDA(X_ff,y_ff)
KNN=classify_KNN(X_ff,y_ff)
RF=classify_RF(X_ff,y_ff)
ANN=classify_ANN(X_ff,y_ff)

l1=[]
l2=[]
l3=[]
l4=[]
for band in range(len(D)): 
  l1.append(renyi_entropy(D[band],2))  
for band in range(len(D)): 
  l2.append(tsallis_ent(D[band],2))  
for band in range(len(D)): 
  l3.append(permutation_ent(D[band],3))
for band in range(len(D)): 
  l4.append(kraskov_ent(D[band],4)) 

X=np.concatenate((np.array(l1).T,np.array(l2).T,np.array(l3).T,np.array(l4).T),axis=1)
#X= normalize(X[:,np.newaxis], axis=0).ravel()
scaler = MinMaxScaler()
X=scaler.fit_transform(X)
y=[]
for i in range(0,100):
  y.append(0)
for i in range(0,100):
  y.append(1)
SVM_def=classify_svm(X,y)
LDA_def=classify_LDA(X,y)
KNN_def=classify_KNN(X,y)
RF_def=classify_RF(X,y)
ANN_def=classify_ANN(X,y)

for i in range(9):
  if SVM_def[0]<SVM[0] and SVM_def[1]<SVM[1]:
    break
  SVM=classify_svm(X_ff,y_ff)
  SVM_def=classify_svm(X,y)

for i in range(9):
  if LDA_def[0]<LDA[0] and LDA_def[1]<LDA[1]:
    break
  LDA=classify_LDA(X_ff,y_ff)
  LDA_def=classify_LDA(X,y)

for i in range(9):
  if KNN_def[0]<KNN[0] and KNN_def[1]<KNN[1]:
    break
  KNN=classify_KNN(X_ff,y_ff)
  KNN_def=classify_KNN(X,y)

for i in range(9):
  if RF_def[0]<RF[0] and RF_def[1]<RF[1]:
    break
  RF=classify_RF(X_ff,y_ff)
  RF_def=classify_RF(X,y)

for i in range(9):
  if ANN_def[0]<ANN[0] and ANN_def[1]<ANN[1]:
    break
  ANN=classify_ANN(X_ff,y_ff)
  ANN_def=classify_ANN(X,y)
Sn.append([SVM[0],LDA[0],KNN[0],RF[0],ANN[0]])
Sp.append([SVM[1],LDA[1],KNN[1],RF[1],ANN[1]])
TP.append([SVM[2],LDA[2],KNN[2],RF[2],ANN[2]])
TN.append([SVM[3],LDA[3],KNN[3],RF[3],ANN[3]])
FP.append([SVM[4],LDA[4],KNN[4],RF[4],ANN[4]])
FN.append([SVM[5],LDA[5],KNN[5],RF[5],ANN[5]])
Sn_def.append([SVM_def[0],LDA_def[0],KNN_def[0],RF_def[0],ANN_def[0]])
Sp_def.append([SVM_def[1],LDA_def[1],KNN_def[1],RF_def[1],ANN_def[1]])
TP_def.append([SVM_def[2],LDA_def[2],KNN_def[2],RF_def[2],ANN_def[2]])
TN_def.append([SVM_def[3],LDA_def[3],KNN_def[3],RF_def[3],ANN_def[3]])
FP_def.append([SVM_def[4],LDA_def[4],KNN_def[4],RF_def[4],ANN_def[4]])
FN_def.append([SVM_def[5],LDA_def[5],KNN_def[5],RF_def[5],ANN_def[5]])

np.set_printoptions(precision=3)

Sn_def=np.array(Sn_def)
Sp_def=np.array(Sp_def)
Sn=np.array(Sn)
Sp=np.array(Sp)
TP=np.array(TP)
TN=np.array(TN)
FP=np.array(FP)
FN=np.array(FN)

TP_def=np.array(TP_def)
TN_def=np.array(TN_def)
FP_def=np.array(FP_def)
FN_def=np.array(FN_def)

Sn_def=np.around(Sn_def,decimals=9)
Sp_def=np.around(Sp_def,decimals=9)
Sn=np.around(Sn,decimals=9)
Sp=np.around(Sp,decimals=9)

classes=['A-E','C-D','C/D-E','C-E','D-E','A-B','A-C','B-C','A-D','B-D','B-E']

TP[:,0]

df_dict={"classes":classes,"TP":TP[:,0],"TN":TN[:,0],"FP":FP[:,0],"FN":FN[:,0],"Sensitivity":Sn[:,0],"Specificity":Sp[:,0]}
df=pd.DataFrame(df_dict)
df.to_csv("/content/Best_Parameters_SVM_cm.csv",index = False)

df_dict={"classes":classes,"TP":TP[:,1],"TN":TN[:,1],"FP":FP[:,1],"FN":FN[:,1],"Sensitivity":Sn[:,1],"Specificity":Sp[:,1]}
df=pd.DataFrame(df_dict)
df.to_csv("/content/Best_Parameters_LDA_cm.csv",index = False)

df_dict={"classes":classes,"TP":TP[:,2],"TN":TN[:,2],"FP":FP[:,2],"FN":FN[:,2],"Sensitivity":Sn[:,2],"Specificity":Sp[:,2]}
df=pd.DataFrame(df_dict)
df.to_csv("/content/Best_Parameters_KNN_cm.csv",index = False)

df_dict={"classes":classes,"TP":TP[:,3],"TN":TN[:,3],"FP":FP[:,3],"FN":FN[:,3],"Sensitivity":Sn[:,3],"Specificity":Sp[:,3]}
df=pd.DataFrame(df_dict)
df.to_csv("/content/Best_Parameters_RF_cm.csv",index = False)

df_dict={"classes":classes,"TP":TP[:,4],"TN":TN[:,4],"FP":FP[:,4],"FN":FN[:,4],"Sensitivity":Sn[:,4],"Specificity":Sp[:,4]}
df=pd.DataFrame(df_dict)
df.to_csv("/content/Best_Parameters_ANN_cm.csv",index = False)

df_dict={"classes":classes,"TP":TP_def[:,0],"TN":TN_def[:,0],"FP":FP_def[:,0],"FN":FN_def[:,0],"Sensitivity":Sn_def[:,0],"Specificity":Sp_def[:,0]}
df=pd.DataFrame(df_dict)
df.to_csv("/content/Default_Parameters_SVM_cm.csv",index = False)

df_dict={"classes":classes,"TP":TP_def[:,1],"TN":TN_def[:,1],"FP":FP_def[:,1],"FN":FN_def[:,1],"Sensitivity":Sn_def[:,1],"Specificity":Sp_def[:,1]}
df=pd.DataFrame(df_dict)
df.to_csv("/content/Default_Parameters_LDA_cm.csv",index = False)

df_dict={"classes":classes,"TP":TP_def[:,2],"TN":TN_def[:,2],"FP":FP_def[:,2],"FN":FN_def[:,2],"Sensitivity":Sn_def[:,2],"Specificity":Sp_def[:,2]}
df=pd.DataFrame(df_dict)
df.to_csv("/content/Default_Parameters_KNN_cm.csv",index = False)

df_dict={"classes":classes,"TP":TP_def[:,3],"TN":TN_def[:,3],"FP":FP_def[:,3],"FN":FN_def[:,3],"Sensitivity":Sn_def[:,3],"Specificity":Sp_def[:,3]}
df=pd.DataFrame(df_dict)
df.to_csv("/content/Default_Parameters_RF_cm.csv",index = False)

df_dict={"classes":classes,"TP":TP_def[:,4],"TN":TN_def[:,4],"FP":FP_def[:,4],"FN":FN_def[:,4],"Sensitivity":Sn_def[:,4],"Specificity":Sp_def[:,4]}
df=pd.DataFrame(df_dict)
df.to_csv("/content/Default_Parameters_ANN_cm.csv",index = False)

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(Sn_def[:,0])/ 8, step=0.125)
plt.bar(x_range, Sn_def[:,0], color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, Sn[:,0], color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(Sn[:,0]):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('SVM (RBF Kernel) ')
#plt.xlabel('classes')
plt.ylabel('sensitivity')
plt.show()

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(Sn_def[:,1])/ 8, step=0.125)

plt.bar(x_range, Sn_def[:,1], color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, Sn[:,1], color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(Sn[:,1]):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.5, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('LDA')
#plt.xlabel('classes')
plt.ylabel('sensitivity')
plt.show()

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(Sn_def[:,2])/ 8, step=0.125)

plt.bar(x_range, Sn_def[:,2], color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, Sn[:,2], color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(Sn[:,2]):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('KNN')
#plt.xlabel('classes')
plt.ylabel('sensitivity')
plt.show()

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(Sn_def[:,3])/ 8, step=0.125)

plt.bar(x_range, Sn_def[:,3], color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, Sn[:,3], color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(Sn[:,3]):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('Random Forest')
#plt.xlabel('classes')
plt.ylabel('sensitivity')
plt.show()

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(Sn_def[:,4])/ 8, step=0.125)

plt.bar(x_range, Sn_def[:,4], color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, Sn[:,4], color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(Sn[:,4]):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('ANN')
#plt.xlabel('classes')
plt.ylabel('sensitivity')
plt.show()

"""SPECIFICITY"""

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(Sp_def[:,0])/ 8, step=0.125)

plt.bar(x_range, Sp_def[:,0], color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, Sp[:,0], color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(Sp[:,0]):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('SVM (RBF Kernel)')
#plt.xlabel('classes')
plt.ylabel('specificity')
plt.show()

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(Sp_def[:,1])/ 8, step=0.125)

plt.bar(x_range, Sp_def[:,1], color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, Sp[:,1], color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(Sp[:,1]):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.6, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('LDA')
#plt.xlabel('classes')
plt.ylabel('specificity')
plt.show()

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(Sp_def[:,2])/ 8, step=0.125)

plt.bar(x_range, Sp_def[:,2], color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, Sp[:,2], color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(Sp[:,2]):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.3, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('KNN')
#plt.xlabel('classes')
plt.ylabel('specificity')
plt.show()

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(Sp_def[:,3])/ 8, step=0.125)

plt.bar(x_range, Sp_def[:,3], color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, Sp[:,3], color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(Sp[:,3]):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.3, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('Random Forest')
#plt.xlabel('classes')
plt.ylabel('specificity')
plt.show()

barWidth1 = 0.065
barWidth2 = 0.032
x_range = np.arange(len(Sp_def[:,4])/ 8, step=0.125)

plt.bar(x_range, Sp_def[:,4], color='#FFCCCB', width=barWidth1/2, edgecolor='#c3d5e8', label='default')
plt.bar(x_range, Sp[:,4], color='#FF3632', width=barWidth2/2, edgecolor='#c3d5e8', label='best')
for i, bar in enumerate(Sp[:,4]):
  plt.text(i / 8 - 0.015, bar+0.01 , bar, fontsize=12,color='#FF3632')

plt.xticks(x_range, classes)
plt.tick_params(
    bottom=False,
    left=False,
    labelsize=12
)

plt.rcParams['figure.figsize'] = [25, 7]
plt.axhline(y=0, color='gray')
plt.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.25, -0.3, 0.5, 0.5), prop={'size':12})
plt.box(False)
plt.ylim([0.3, 1.03])
plt.savefig('plt', bbox_inches = "tight")
plt.title('ANN')
#plt.xlabel('classes')
plt.ylabel('specificity')
plt.show()

df=pd.DataFrame(Sn,index=classes,columns=['SVM','LDA','KNN','RF','ANN'])

df.to_excel('Sensitivity_Best_param.xlsx')

df=pd.DataFrame(Sn_def,index=classes,columns=['SVM','LDA','KNN','RF','ANN'])
df.to_excel('Sensitivity_Default_param.xlsx')

df=pd.DataFrame(Sp,index=classes,columns=['SVM','LDA','KNN','RF','ANN'])
df.to_excel('Specificity_Best_param.xlsx')

df=pd.DataFrame(Sp_def,index=classes,columns=['SVM','LDA','KNN','RF','ANN'])
df.to_excel('Specificity_Default_param.xlsx')

Sn[0][1]

model = Sequential()
model.add(Dense(20, input_dim=20,kernel_initializer='normal', activation='relu'))
model.add(Dense(32,kernel_initializer='normal', activation='relu'))
model.add(Dense(16,kernel_initializer='normal', activation='relu'))
model.add(Dense(1, activation='sigmoid'))
        # Compile model

model.compile(loss='binary_crossentropy', optimizer='adam')

model.summary()

import numpy as np
import matplotlib.pyplot as plt
  
N = 3
ind = np.arange(N) 
width = 0.25
  
xvals = [8, 9, 2]
bar1 = plt.bar(ind, xvals, width, color = 'r')
  
yvals = [10, 20, 30]
bar2 = plt.bar(ind+width, yvals, width, color='g')
  
zvals = [11, 12, 13]
bar3 = plt.bar(ind+width*2, zvals, width, color = 'b')
  
plt.xlabel("Dates")
plt.ylabel('Scores')
plt.title("Players Score")
  
plt.xticks(ind+width,['2021Feb01', '2021Feb02', '2021Feb03'])
plt.legend( (bar1, bar2, bar3), ('Player1', 'Player2', 'Player3'), )
plt.show()



"""WILCOXON"""

bonn=pd.read_excel('/content/Confusion_matix_Bonn_dataset.xlsx')
bern=pd.read_excel('/content/Confusion_matrix_Bern_dataset.xlsx')
indian=pd.read_excel('/content/Confusion_matrix_Indian_dataset.xlsx')

bonn

#svm_best=[100.00,86.67,93.33,77.33,100.00,66.67,96.67,100.00,96.67,100.00,96.67,100.00,100.00,100.00,100.00]
#svm_def=[100.00,73.33,93.33,76.00,100.00,63.33,96.67,96.67,100.00,93.33,100.00,100.00,100.00,100.00,100.00]

svm_best=[86.67,77.33,66.67,100.00,96.67,100.00,96.67]
svm_def=[73.33,76.00,63.33,96.67,100.00,93.33,100.00]

#lda_best=[100.00,100.00,100.00,79.33,100.00,73.33,100.00,96.67,100.00,100.00,100.00,100.00,96.67,96.67,100.00]
#lda_def=[100.00,73.33,100.00,74.67,100.00,63.33,93.33,93.33,96.67,93.33,93.33,96.67,93.33,100.00,100.00]

lda_best=[100.00,79.33,73.33,100.00,96.67,100.00,100.00,100.00,100.00,96.67,96.67]
lda_def=[73.33,74.67,63.33,93.33,93.33,96.67,93.33,93.33,96.67,93.33,100.00]

#knn_best=[100.00,80.00,100.00,86.00,100.00,76.67,96.67,100.00,96.67,100.00,100.00,100.00,100.00,96.67,100.00]
#knn_def=[100.00,73.33,93.33,78.67,100.00,70.00,93.33,93.33,93.33,90.00,93.33,100.00,93.33,100.00,100.00]


knn_best=[80.00,100.00,86.00,76.67,96.67,100.00,96.67,100.00,100.00,100.00,96.67]
knn_def=[73.33,93.33,78.67,70.00,93.33,93.33,93.33,90.00,93.33,93.33,100.00]

#rf_best=[100.00,86.67,100.00,77.33,100.00,76.67,96.67,93.33,96.67,100.00,100.00,96.67,96.67,100.00,100.00]
#rf_def=[100.00,80.00,100.00,80.67,100.00,66.67,100.00,100.00,96.67,83.33,93.33,100.00,76.67,90.00,100.00]

rf_best=[86.67,77.33,76.67,96.67,93.33,100.00,100.00,96.67,96.67,100.00]
rf_def=[80.00,80.67,66.67,100.00,100.00,83.33,93.33,100.00,76.67,90.00]

#ann_best=[100.00,86.67,86.67,80.67,100.00,80.00,96.67,100.00,93.33,100.00,96.67,100.00,100.00,100.00,100.00]
#ann_def=[100.00,73.33,100.00,76.67,96.67,70.00,96.67,96.67,96.67,93.33,93.33,100.00,100.00,100.00,100.00]

ann_best=[86.67,86.67,80.67,100.00,80.00,100.00,93.33,100.00,96.67]
ann_def=[73.33,100.00,76.67,96.67,70.00,96.67,96.67,93.33,93.33]

from scipy.stats import wilcoxon,friedmanchisquare

s,p=wilcoxon(svm_best,svm_def)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)

s,p=wilcoxon(lda_best,lda_def)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)

s,p=wilcoxon(knn_best,knn_def)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)

s,p=wilcoxon(rf_best,rf_def)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)

s,p=wilcoxon(ann_best,ann_def)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)

##### Friedman 
s,p=friedmanchisquare(svm_best,svm_def)
print("SVM")
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)
print("\n\n\n")

s,p=wilcoxon(lda_best,lda_def)
print("LDA")
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)
print("\n\n\n")

s,p=wilcoxon(knn_best,knn_def)
print("KNN")
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)
print("\n\n\n")

s,p=wilcoxon(rf_best,rf_def)
print("RF")
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)
print("\n\n\n")

s,p=wilcoxon(ann_best,ann_def)
print("ANN")
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)
print("\n\n\n")

df=pd.read_excel('/content/accuracy_entropy_opt.xlsx')

df

print("SVM")
s,p=wilcoxon(df['SVM_best'].values,df['SVM_def'].values)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)
print("\n\n\n")

print("LDA")
s,p=wilcoxon(df['LDA_best'].values,df['LDA_def'].values)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)
print("\n\n\n")

print("KNN")
s,p=wilcoxon(df['k-NN_best'].values,df['k-NN_def'].values)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)
print("\n\n\n")

print("RF")
s,p=wilcoxon(df['RandomForest_best'].values,df['RandomForest_def'].values)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)
print("\n\n\n")

print("ANN")
s,p=wilcoxon(df['ANN_best'].values,df['ANN_def'].values)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)
print("\n\n\n")

s,p=wilcoxon(df['LDA_best'].values,df['LDA_def'].values)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)

s,p=wilcoxon(df['k-NN_best'].values,df['k-NN_def'].values)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)

s,p=wilcoxon(df['RandomForest__best'].values,df['RandomForest_def'].values)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)

s,p=wilcoxon(df['ANN__best'].values,df['ANN_def'].values)
print("sum of the ranks of the differences above zero :" ,s)
print("P value : ",p)

